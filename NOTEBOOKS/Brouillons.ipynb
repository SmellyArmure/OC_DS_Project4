{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "df_res = pd.DataFrame(dtype = 'object')\n",
    "\n",
    "dict_param_grid = {'KNN': {'n_neighbors': [1,2,3,5,7,9,11]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting parameters\n",
    "reg = KNeighborsRegressor()\n",
    "name_reg = 'KNN'\n",
    "param_grid = {str(reg)[:-2].lower()+'__'+k : v \\\n",
    "              for k,v in dict_param_grid[name_reg].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# researching best hyperparameters and fitting on training set\n",
    "pipe = make_pipeline(column_trans, reg)\n",
    "gscv = GridSearchCV(pipe,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=kf, verbose=1)\n",
    "gscv.fit(X_tr,y1_tr)\n",
    "\n",
    "# best hyperparams\n",
    "df_res = pd.DataFrame(dtype = 'object')\n",
    "df_res.loc['name_params', name_reg] = tuple([k for k,v in dict_param_grid[name_reg].items()])\n",
    "df_res.loc['best_params', name_reg] = tuple([gscv.best_params_[p] for p in param_grid])\n",
    "\n",
    "# score of the model with best params on testing set\n",
    "y_pr = gscv.predict(X_te)\n",
    "res = scores_reg(name_reg, X_te, y1_te, y_pr).astype('object')\n",
    "df_res = df_res.append(res.to_frame())\n",
    "\n",
    "# mean cv score of the model with best params on testing set\n",
    "res = cv_scores_reg(name_reg, gscv.best_estimator_, X_te, y1_te, cv=6).astype('object')\n",
    "df_res = df_res.append(res.to_frame())\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting optimisation curve of hyperparameters\n",
    "\n",
    "\n",
    "list_n_params = [n for n in gscv_res.keys()if 'param_' in n]\n",
    "list_params = [gscv.cv_results_[p].tolist() for p in list_n_params]\n",
    "for n_p, li_p in zip(list_n_params, list_params):\n",
    "    plt.errorbar(li_p,\n",
    "            gscv.cv_results_['mean_test_score'],\n",
    "            yerr=gscv.cv_results_['std_test_score'])\n",
    "    plt.gcf().suptitle(n_p)\n",
    "# plt.gca().set_xscale('log')\n",
    "# plt.gca().set(xlim=(0,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "name_reg = 'LinReg(allcols)'\n",
    "\n",
    "pipe = make_pipeline(column_trans, reg)\n",
    "pipe.fit(X_tr,y1_tr)\n",
    "\n",
    "y_pr = pipe.predict(X_te)\n",
    "\n",
    "res = scores_reg(name_reg, X_te, y1_te, y_pr).astype('object')\n",
    "df_lr = df_lr.append(res.to_frame())\n",
    "res = cv_scores_reg(name_reg, pipe,\n",
    "                    X_te, y1_te, cv=6).astype('object')\n",
    "df_lr = df_lr.append(res.to_frame())\n",
    "\n",
    "df_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting optimisation curve of hyperparameters\n",
    "param_grid = dict_param_grid[name_reg]\n",
    "\n",
    "gscv_res = knn_gscv.cv_results_\n",
    "list_n_params = [n for n in gscv_res.keys()if 'param_' in n]\n",
    "list_params = [gscv_res[p].tolist() for p in list_n_params]\n",
    "for n_p, li_p in zip(list_n_params, list_params):\n",
    "    plt.errorbar(li_p,\n",
    "            gscv_res['mean_test_score'],\n",
    "            yerr=gscv_res['std_test_score'])\n",
    "    plt.gcf().suptitle(n_p)\n",
    "# plt.gca().set_xscale('log')\n",
    "# plt.gca().set(xlim=(0,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = ElasticNet()\n",
    "name_reg = 'my_enet'\n",
    "dict_param_grid = {'my_enet': {'alpha': [1,15],\n",
    "                               'l1_ratio': [0.3,0.5]}}\n",
    "kf_gscv = KFold(n_splits=2, shuffle=True)\n",
    "\n",
    "# version unique\n",
    "enet_gscv, new_df_res = model_optimizer(name_reg=name_reg, data_preproc=column_trans,\n",
    "                                 reg=reg, param_grid=dict_param_grid[name_reg],\n",
    "                                 Xtr=X_tr, ytr=y1_tr, Xte=X_te, yte=y1_te,\n",
    "                                 cv_gs=kf_gscv, cv_test=6)\n",
    "df_res = pd.concat([df_res, new_df_res], axis=1)\n",
    "\n",
    "display(df_res)\n",
    "\n",
    "plot_2D_hyperparam_opt(enet_gscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "name_reg = 'LinReg(allcols)'\n",
    "\n",
    "pipe = make_pipeline(column_trans, reg)\n",
    "pipe.fit(X_tr,y1_tr)\n",
    "\n",
    "y_pr = pipe.predict(X_te)\n",
    "\n",
    "ser = scores_reg(name_reg, X_te_sel, y1_te, y_pr).astype('object')\n",
    "ser = ser.append(cv_scores_reg(name_reg, pipe, X_te_sel, y1_te, cv=6).astype('object'))\n",
    "\n",
    "df_lr = df_lr.append(ser.to_frame())\n",
    "\n",
    "df_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_reg = name_reg\n",
    "# pipe= pipe\n",
    "# Xte = X_te\n",
    "# yte = y1_te\n",
    "# df_res=df_lr\n",
    "# cv=5\n",
    "\n",
    "# if df_res is None:\n",
    "#     df_res = pd.DataFrame(dtype = 'object')\n",
    "# df_res_mod = pd.DataFrame(dtype = 'object')\n",
    "# ypr = pipe.predict(Xte)\n",
    "# ser = scores_reg(name_reg, Xte, yte, ypr).astype('object')\n",
    "# ser = ser.append(cv_scores_reg(name_reg, pipe, Xte, yte, cv=cv).astype('object'))\n",
    "# df_res_mod = pd.concat([df_res,ser.to_frame()],1)\n",
    "# df_res_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_append_curve(x, y, c, fig=None):\n",
    "    %matplotlib\n",
    "    if fig is None:\n",
    "        fig = plt.figure()\n",
    "    ax = fig.axes[0]\n",
    "    ax.plot(x, y, c=c)\n",
    "    %matplotlib inline\n",
    "    return fig\n",
    "\n",
    "fig = plot_append_curve([100,11,90,13], [100,90,70,200], c='grey', fig=fig);\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD VERSION\n",
    "class CustEncoder(BaseEstimator) :\n",
    "\n",
    "    def __init__(self, l_card_cols, h_card_cols, bool_cols,\n",
    "                 strat_low_card='ord', strat_high_card='ord'):\n",
    "        self.l_card_cols = l_card_cols\n",
    "        self.h_card_cols = h_card_cols\n",
    "        self.bool_cols = bool_cols\n",
    "        self.strat_low_card = strat_low_card\n",
    "        self.strat_high_card = strat_high_card\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Dictionary to translate strategies\n",
    "        d_enc = {'ohe': ce.OneHotEncoder(),\n",
    "                 'hash': ce.HashingEncoder(),\n",
    "                 'ord': ce.OrdinalEncoder(),\n",
    "                 'loo': ce.LeaveOneOutEncoder(),\n",
    "                 'bin': ce.BinaryEncoder()}\n",
    "\n",
    "        # Creates a columns transformer with chosen strategies\n",
    "        self.column_trans = ColumnTransformer([(\"low_card\", d_enc[self.strat_low_card], l_card_cols),\n",
    "                                       (\"bool\", ce.OrdinalEncoder(), bool_cols),\n",
    "                                       (\"high_card\", d_enc[self.strat_high_card], h_card_cols)],\n",
    "                                       remainder=StandardScaler())\n",
    "        return self.column_trans.fit(X, y)\n",
    "  \n",
    "    def transform(self, X, y=None):\n",
    "        return  self.column_trans.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = knn_gscv3.cv_results_\n",
    "scoring = ['neg_root_mean_squared_error', 'r2']\n",
    "param = 'param_KNN3__n_neighbors'\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "          fontsize=16)\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"Score\")\n",
    "ax = plt.gca()\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results[param].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(sorted(scoring), ['g', 'k']):\n",
    "    for sample, style in (('train', '--'), ('test', '-')):\n",
    "        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                        sample_score_mean + sample_score_std,\n",
    "                        alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "        ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                alpha=1 if sample == 'test' else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "    best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score, \n",
    "                (X_axis[best_index]+0.5, best_score + 0.005))\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
